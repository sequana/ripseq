#
#  This file is part of Sequana software
#
#  Copyright (c) 2016-2021 - Sequana Dev Team (https://sequana.readthedocs.io)
#
#  Distributed under the terms of the 3-clause BSD license.
#  The full license is in the LICENSE file, distributed with this software.
#
#  Website:       https://github.com/sequana/sequana
#  Documentation: http://sequana.readthedocs.io
#  Contributors:  https://github.com/sequana/sequana/graphs/contributors
##############################################################################

"""ripseq pipeline

Author: Sequana Developers

This pipeline is part of Sequana project (sequana.readthedocs.io)

Execution:
1. prepare the pipelie:

    sequana_ripseq --help

2. exceute it either manually:

    snakemake -s ripseq.rules --forceall --stats stats.txt --cores 4

or following the instructions from step 1.
"""
import glob
import pyBigWig
from tqdm import tqdm
import pandas as pd

from sequana_pipetools import snaketools as sm

from sequana_pipelines.ripseq.tools import get_data

# This must be defined before the include
configfile: "config.yaml"

# A convenient manager
manager = sm.PipelineManager("ripseq", config)

#
#Pipeline valid for 4 replicates
#

CASES = ["24h", "48h", "Mock"]
CHROMS = [str(x) for x in range(1,23)] + ['X', 'Y']


rule pipeline:
    input:
        expand("results/G4Hunter/analysis/{chrom}_candidates-Merged.txt", chrom=CHROMS),
        "results/G4Hunter/G4hunter.csv",
        ".sequana/rulegraph.svg"


rule build_normalisation:
    input:
        config["build_normalisation"]["multiqc_fastqc_file"]
    output:
        "normalisation.txt"
    run:
        from sequana_pipelines.ripseq.tools import build_normalisation
        bn = build_normalisation(input[0], output[0])


def get_ip_files(wildcards):
    return sorted(glob.glob(f"../data/{wildcards.case}*IP*.bw"))


def get_input_files(wildcards):
    return sorted(glob.glob(f"../data/{wildcards.case}*INPUT*.bw"))


rule compute_correlations:
    input:
        ip_files=get_ip_files,
        in_files=get_input_files,
        normalisation="normalisation.txt"
    output:
        csv="results/{case}/correlations/{chrom}.csv"
    params:
        step=config["correlations"]["step"]
    run:
        IPs = []
        for filename in input.ip_files:
            IPs.append(pyBigWig.open(filename))
        INPUTs = []
        for filename in input.in_files:
            INPUTs.append(pyBigWig.open(filename))

        df = get_data(
            IPs,
            INPUTs,
            wildcards.chrom,
            wildcards.case,
            step=params.step,
            norm_file=input.normalisation)
        df.to_csv(output.csv, index=False)


rule get_events:
    input:
        results="results/{case}/correlations/{chrom}.csv",
        gff=config['annotation_file']
    output:
        "results/{case}/events/{chrom}.csv"
    params:
        step=config['correlations']['step'],
        CIP=config['detection']['CIP'],
        min_IP=config['detection']['min_IP'],
        ratio=config['detection']['ratio']
    run:
        from sequana import GFF3
        gff = GFF3(input.gff)
        from sequana_pipelines.ripseq.tools import get_events
        gff.df['gene_name'] = gff.df['Name']
        dd = get_events(
            input.results,
            wildcards.chrom,
            params.step,
            gff,
            CIP=params.CIP,
            min_IP=params.min_IP,
            ratio=params.ratio,
            mycase=wildcards.case)
        dd.to_csv(output[0], index=False)


rule plot_grouped:
    """Plot a peak overview for each significant candidate"""
    input:
        csv = rules.get_events.output[0],
        ip_files=get_ip_files,
        in_files=get_input_files,
        normalisation="normalisation.txt"
    output:
        "results/{case}/images/{chrom}.done"
    params:
        step = 1000
    run:
        from sequana_pipelines.ripseq.tools import plot_grouped
        import pandas as pd
        from pylab import clf, savefig, title, ylim, fill_between
        df = pd.read_csv(input.csv)
        case = wildcards.case
        import numpy as np
        import pyBigWig
        IPs = [pyBigWig.open(x) for x in input.ip_files]
        INPUTs = [pyBigWig.open(x) for x in input.in_files]

        datanorm = pd.read_csv(input.normalisation)
        for _, row in df.iterrows():
            position = int(row['position'])
            start = str(row['start'])
            stop = str(row['stop'])
            try:
                chrom = str(int(row['chrom']))
            except:
                chrom = str(row['chrom'])
            clf()
            plot_grouped(IPs, INPUTs, chrom, position, position + params.step, datanorm)
            CIP = round(row['CIP'], 2)
            ratio = round(row['ratio'], 2)
            mmIP = round(row['mmIP'], 2)

            def is_nan(x):
                return isinstance(x, (float, np.floating)) and np.isnan(x)

            if not is_nan(row['gene_id']):
                geneid = ",".join(row['gene_name'].split("/"))
                title(f"{chrom}:{position}-{position+params.step}; CIP={CIP} ratio={ratio}, mmIP={mmIP}\n{geneid}", fontsize=12)
            else:
                title(f"{chrom}:{position}-{position+params.step}; CIP={CIP} ratio={ratio}, mmIP={mmIP}", fontsize=12)
            import os
            os.makedirs(f"results/{case}/images", exist_ok=True)
            savefig(f"results/{case}/images/{chrom}_{position}.png")
        with open(f"results/{case}/images/{chrom}.done", "w") as fout:
            fout.write("")


rule unique_candidates:
    input:
        csv=expand("results/{case}/events/{chrom}.csv", case=CASES, chrom=CHROMS)
    output:
        csv="results/unique_candidates.csv"
    run:

        events = []
        for case in CASES:
            filenames = glob.glob(f"results/{case}/events/*.csv")
            for filename in filenames:
                df = pd.read_csv(filename)
                df['case'] = case
                events.append(df)
        candidates = pd.concat(
            events,
            axis=0)
        candidates = candidates[~candidates[['chrom', 'position']].duplicated()]
        candidates.to_csv(output.csv, index=False)


rule plot_grouped_3cases:
    input:
        csv = expand("results/{case}/events/{chrom}.csv", case=CASES, chrom=CHROMS),
        candidates=rules.unique_candidates.output.csv,
        normalisation="normalisation.txt"
    output:
        "results/All/done"
    run:
        from sequana_pipelines.ripseq.tools import plot_grouped
        import pandas as pd
        from pylab import clf, savefig, title, ylim, fill_between   
        import pyBigWig
        import numpy as np

        final = {}
        step = 1000
        window_side = 1000

        all_IPs = {}
        all_INs = {}
        all_dfs = {}
        for case in CASES:
            import pyBigWig
            IPs = glob.glob(f"../data/{case}*IP*.bw")
            INPUTs = glob.glob(f"../data/{case}*INPUT*.bw")
            all_IPs[case] = [pyBigWig.open(x) for x in IPs]
            all_INs[case] = [pyBigWig.open(x) for x in INPUTs]
 

        results = []    
        candidates = pd.read_csv(input.candidates)

        datanorm = pd.read_csv(input.normalisation)
        from tqdm import tqdm
        for _, candidate in tqdm(candidates.iterrows()):
            position = int(candidate['position'])
            try:
                chrom = str(int(candidate['chrom']))
            except:
                chrom = str(candidate['chrom'])
            result = []
            result.append(position)
            result.append(chrom)

            for case in CASES:

                clf()
                plot_grouped(all_IPs[case], all_INs[case], chrom, position, position+window_side, datanorm)
                pos = int(position / step)
                CIP_t = round(candidate.CIP, 2)
                ratio_t = round((candidate.mmIP) / (1+candidate.mmIN),2)
                mmIP_t = round(candidate.mmIP, 2)
                result.append(CIP_t)
                result.append(candidate.mmIP)
                result.append(candidate.mmIN)
                result.append(ratio_t)

                def is_nan(x):
                    return isinstance(x, (float, np.floating)) and np.isnan(x)

                if not is_nan(candidate['gene_id']):
                    gene_names = candidate['gene_name']
                    title(f"{chrom}:{position}-{position+step}; CIP={CIP_t} ratio={ratio_t}, mmIP={mmIP_t}\n{gene_names}", fontsize=12)
                else:
                    title(f"{chrom}:{position}-{position+step}; CIP={CIP_t} ratio={ratio_t}, mmIP={mmIP_t}", fontsize=12)
                import os
                os.makedirs(f"results/All", exist_ok=True)
                os.makedirs(f"results/All/{case}", exist_ok=True)
                savefig(f"results/All/{case}/{chrom}_{position}.png")

            gene_names = candidate['gene_name']
            gene_ids = candidate['gene_id']
            starts = candidates['start']
            stops = candidates['stop']
            result.append(gene_names)
            result.append(gene_ids)
            result.append(starts)
            result.append(stops)
            results.append(result)

        with open(output[0], "w") as fout:
            fout.write("")
        df = pd.DataFrame(results)

        # order must follows ther CASES. FIXE
        df.columns = ["position", "chrom", 
              "24h_CIP", "24h_mmIP", "24h_mmIN", "24h_ratio", 
              "48h_CIP", "48h_mmIP", "48h_mmIN", "48h_ratio",
              "Mock_CIP", "Mock_mmIP", "Mock_mmIN", "Mock_ratio",
              "gene_names", "gene_ids", "starts", "stops"]

        df.to_csv("results/All/all.csv")


rule build_fasta:
    input:
        fasta = config['reference_file'],
        candidates = "results/unique_candidates.csv"
    output:
        expand("results/G4Hunter/fasta/{chrom}_candidates.fa", chrom=CHROMS)
    params:
        step=1000,
        delta=20
    run:
        from sequana import FastA
        f = FastA(input.fasta)

        candidates = pd.read_csv(input.candidates)
        for a in f:
            chrom = a.name
            if chrom.startswith("GL") is False:
                with open(f"results/G4Hunter/fasta/{chrom}_candidates.fa", "w") as fout:
                    for x in candidates.query("chrom==@chrom").position.values:
                        seq = a.sequence[x-params.delta:x+params.step+params.delta]  #+- 20 for window size
                        fout.write(f">{chrom}_{x}\n{seq}\n")


rule G4:
    input:
        fasta=expand("results/G4Hunter/fasta/{chrom}_candidates.fa", chrom=CHROMS)
    output:
        g4=expand("results/G4Hunter/analysis/{chrom}_candidates-Merged.txt", chrom=CHROMS),
        g4_merged="results/G4Hunter/G4hunter.csv"
    params:
        window=config['G4']['window'],
        score=config['G4']['score']
    run:
        import pandas as pd
        from sequana.G4hunter import G4Hunter
        for filename in input.fasta:
            print(filename)
            chrom=filename.split("/")[-1].split("_")[0]
            g4 = G4Hunter(filename, window=params.window, score=params.score)
            g4.run(f"results/G4Hunter/analysis")

        # merge the results
        from sequana import G4hunter
        g4 = G4hunter.G4HunterReader()
        g4.load_files("results/G4Hunter/analysis//*Merged*txt")

        df = pd.concat(g4.data_merged) 
        df = df.reset_index()
        del df['level_1']
        df['chrom'] = [x.split("_")[0] for x in df['level_0'].values]
        df['position_window'] = [x.split("_")[1] for x in df['level_0'].values]
        df['position'] = [int(x.split("_")[1])  for x in df['level_0'].values] + df['Start'].values
        del df['level_0']
        df = df[["chrom", "position", "position_window", "Start", "End", "Sequence", "Length", "Score", "NBR"]]
        df['Sequence'] = [x.upper() for x in df['Sequence']]
        df.to_csv(output.g4_merged, sep=",", index=None)
# ========================================================== rulegraph
rule build_rulegraph:
    input: str(manager.snakefile)
    output:
        svg = "rulegraph/rulegraph.dot"
    params:
        mapper = {},
        configname = "config.yaml"
    wrapper:
        f"{manager.wrappers}/wrappers/rulegraph"


rule dot2svg:
    input:
        rules.build_rulegraph.output.svg
    output:
        ".sequana/rulegraph.svg"
    container:
        config['apptainers']['graphviz']
    shell:
        """dot -Tsvg {input} -o {output}"""



# Those rules takes a couple of seconds so no need for a cluster
localrules: build_rulegraph


onsuccess:

    from sequana import logger as log
    from sequana.modules_report.summary import SequanaReport

    import colorlog
    log = colorlog.getLogger("sequana.rnaseq")
    log.setLevel("INFO")

    manager.teardown(
        extra_files_to_remove=["requirements.txt"],
        extra_dirs_to_remove=[".genomes", "tmp", "logs"])

    # Now the final report. add the original command in the HTML report
    intro = ""
    data = manager.getmetadata()
    s = SequanaReport(data, intro)

    shell("chmod -R g+w .")
    shell("rm -rf rulegraph")

onerror:
    manager.onerror()
